# -*- coding: utf-8 -*-
"""fake_comment_classification.ipynb

Automatically generated by Colaboratory.

## Flow in this tutorial.
1. Import Lib
2. Load Dataset. `(train.json & test.json)`
3. Data Pre-processing.
4. BERT Embeddin.
5. Build SVN classifier.

## 1. Import Library
"""

from google.colab import files
import json
import re

import pandas as pd
import numpy as np
from pandas.core.frame import DataFrame
from sklearn.utils import shuffle

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

"""You will have to install `transformers` library each time you reconnect."""

!pip install transformers 
from transformers import BertTokenizer, BertModel,BertConfig

"""##2. Load Dataset
1. Create `upload_file_to_colab()` function for upload files `(train.json & test.json)` to `colab`.
2. Call function and upload files.
"""

# Step 1.
def upload_file_to_colab():
  uploaded = files.upload()
  for fileName in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes.'.format(name=fileName, length=len(uploaded[fileName])))

# Step 2.
# upload train.json
print('Upload train.json please.\n')
upload_file_to_colab()

# upload test.json
print('Upload test.json please.\n')
upload_file_to_colab()

"""## 3. Data Pre-processing
1. Create a funciton `sentence_regex()` for do `Regular Expression`. When putting a sentence in this function, return a sentence that finished regular expression
2. Create `remove_empty_row()` function for remove all empty rows from dataframe.
3. Create `label_to_spam()` function for label data. If the value of column(`is_spam`) is `True`, return 1 else 0. 
4. Create a function `preprocessing_data()`. That is the main function of our pre-processing step. In this function, it will call all the above to process all sentences.
5. Call `preprocessing_data()` twice for both `test.json` and `train.json`.
6. Create dictionary objects for easy conversion to dataframe and remove empty rows.
7. Here, we just want to know the number of each label. (0 or 1)
"""

# Step 1.
def sentence_regex(sentence):
  complie = re.compile(u"[\u4e00-\u9fa5]+")

  sentence = sentence.replace("\n", ' ')
  sentence = re.sub(u'[a-zA-Z0-9]', '', sentence)

  sentence = re.findall(complie, sentence) #一則評論只取中文字，分成多行
  return ''.join(sentence)

# Step 2.
def remove_empty_row(df, col='text'):
  emptyTitleFIlter = (
        (df[col].isnull())|(df[col] == '')|(df[col] == '0')
        )
  df = df[~emptyTitleFIlter]
  return df

# Step 3.
def label_to_spam(is_spam):
  if is_spam == True:
      return 1 
  return 0

# step 4.
def preprocessing_data(filePath):
    data = pd.read_json(filePath)

    contents = list()
    spamLabels = list()
    length = len(data)

    # iterate each row, sentence(content) and label(is_spam), by index.
    for idx in range(length):
      sentence = data['content'][idx]
      is_spam = data['is_spam'][idx]

      # do regular expression
      # when finished, append it to a list (contents)
      sentence = sentence_regex(sentence)
      contents.append(sentence)
      
      # encode `is_spam` value.
      # when finished, append it to a list (spamLabels)
      is_spam = label_to_spam(is_spam)
      spamLabels.append(is_spam)

    return contents, spamLabels

# Step 5.
train_contents, train_spamLabels = preprocessing_data('train.json')
print('* Train File:\n\tFirst sentence:', train_contents[0], '\n\tFirst label:', train_spamLabels[0])

test_contents, test_spamLabels = preprocessing_data('test.json')
print('* Test File:\n\tFirst sentence:', test_contents[0], '\n\tFirst label:', test_spamLabels[0])

# Step 6 for train.json data
trainDict = {
    'text': train_contents,
    'label': train_spamLabels
}

trainDf = DataFrame(trainDict)
trainDf = remove_empty_row(trainDf, 'text')
trainDf.head()

# Step 6 for test.json data
testDict = {
    'text': test_contents,
    'label': test_spamLabels
}

testDf = DataFrame(testDict)
testDf = remove_empty_row(testDf, 'text')
testDf.head()

# Step 7.
# define mask(filters)
train_true_filter = trainDf['label'] == 0
train_fake_filter = trainDf['label'] == 1
test_true_filter = testDf['label'] == 0
test_fake_filter = testDf['label'] == 1

true_train_count = len(trainDf.loc[train_true_filter])
true_test_count = len(testDf.loc[test_true_filter])
print('True Comment Count:', true_train_count + true_test_count)

fake_train_count = len(trainDf.loc[train_fake_filter])
fake_test_count = len(testDf.loc[test_fake_filter])
print('Fake Comment Count:', fake_train_count + fake_test_count)

"""So far, the pre-processing of the text has been done. Before we do embedding, we need to merge two dataframe `trainDf` and `testDf`.
1. Create `concat_two_dataframe()` function.
2. Concate `trainDf` and `testDf`.
3. Because Fake's number less than real's, random sampling from True's data and then set the ratio is 3:1.
4. Call `concat_two_dataframe()` function again, as `dataset`, and this time we merge the data sampled. (`sampling_true_data` and `fake_data`)
5. Get all `text` and all `label` from dataset.
"""

# Step 1.
def concat_two_dataframe(df_1, df_2):
  concated = pd.concat([df_1, df_2], axis=0)
  concated = shuffle(concated)
  # reset index
  concated = concated.reset_index(drop=True)
  return concated

# Step 2.1 concated train dataframe and test dataframe
concated_df = concat_two_dataframe(trainDf, testDf)

# Real and Fake Mask (filter)
fake_label_filter = concated_df['label'] == 1
true_label_filter = concated_df['label'] == 0

fake_data = concated_df.loc[fake_label_filter]
true_data = concated_df.loc[true_label_filter]

# Step 2.2 See the number of each label.
print('Real data total number:', len(true_data))
print('Fake data total number:', len(fake_data))

# Step 3.
# Because Fake's number less than real's.
# random sampling from True's data.
# set the ratio is 3:1
sampling_true_data = true_data.sample(
    n=len(fake_data) * 3,
    random_state=5487
  )

# Step 4.
dataset = concat_two_dataframe(sampling_true_data, fake_data)
dataset.info()

# Step 5.
texts = dataset['text'].values
labels = dataset['label'].values

"""## 4. BERT Embedding
We can BERT embeddings now. In this tutorial, we load `bert-base-chinese`.
1. Instance a `tokenizer` object for encode each sentence.
2. Create `encode_sentence_by_bert()` for encoding every sentence place in this function.
3. Define a list(`input_id`) for storing all encoded tokens from BERT's tokenizer.
4. Pad sequences length to 100.
5. Create `MASK` list.
6. Convert data to tensor.
7. Create the `DataLoader` object for our training set.
8. Create BERT model and load BERT config.
9. Check GUP avalible.
10. Iterate batch data from our dataloader, and do embeddings.
"""

# Step 1.
PRETRAINED_MODEL_NAME = "bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

# Step 2.
def encode_sentence_by_bert(sentence):
  encoded_sentence = tokenizer.encode(
        sentence, # Sentence to encode.
        add_special_tokens = True, # add [CLS] and [SEP]
        truncation = True,
        max_length = 100 
    )
  return encoded_sentence

# Step 3.
input_ids = list()

# encode each sentence in texts.
for sentence in texts:
  encoded_sentence = encode_sentence_by_bert(sentence)
  # when sentence was encoded, append to input_ids list.
  input_ids.append(encoded_sentence)

print('Number of sentence:', len(input_ids))
print('Length of encode sentence:', len(input_ids[0]))

# Step 4.
from keras.preprocessing.sequence import pad_sequences
MAX_LEN = 100
input_ids = pad_sequences(
    input_ids, 
    maxlen= MAX_LEN, 
    dtype="long", 
    value=0, # padding by value 0
    truncating="post", # truncating from behind
    padding="post") # padding from behind

# Step 5.
attention_masks = list()
for sentence in input_ids:
    att_mask = [int(token_id > 0) for token_id in sentence]
    attention_masks.append(att_mask)

# Step 6.
inputs = torch.tensor(input_ids)
labels = torch.tensor(labels)
masks = torch.tensor(attention_masks)

# Step 7. Create the DataLoader for our training set.
batch_size = 8
train_dataset = TensorDataset(inputs, masks, labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

# Step 8. 
# Create BERT model
'''
The bare Bert Model transformer outputting raw hidden-states without any specific head on top.
'''
config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)
model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, config=config)
model.cuda()

# Step 9. 
if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

# Step 10.
train_label = list()
train_set = list()

count=0
for step, batch in enumerate(train_dataloader):
  batch_tensor = tuple(tensr.to(device) for tensr in batch)
  # b_ means batch
  b_input_ids, b_input_mask, b_labels = batch_tensor
  outputs = model(b_input_ids, b_input_mask)

  # get last_layer 
  last_layer = outputs[0] # last layer
  label_ids = b_labels.to('cpu').numpy() 
  train_label.append(np.array(label_ids)) # real label for training a SVM model.

  for state in last_layer:
    # state size: (100, 768)
    allWordVecs = list()
    for word_unit in state:
      # size (768)
      allWordVecs.append(word_unit.detach().cpu().numpy())
    train_set.append(allWordVecs)

labels = list()
for i in train_label:
    for j in i:
        labels.append(j)
labels = np.array(labels)
print('Length of Labels:', len(labels))

# Show the shape of train_set.
layer = np.array(train_set)
train_set = None
print(layer.shape)

"""## 5. Build SVN Classifier"""

# split dataset 
X_train, X_test, y_train, y_test = \
    train_test_split(
        layer, 
        labels,
        test_size=0.3,
        random_state=0,
        stratify=labels
    )

# first feature CLS
X_train = X_train[:, 0, :]
X_test = X_test[:, 0, :]
print(X_train.shape)

model_svm = SVC(kernel='rbf')
model_svm.fit(X_train, y_train)
prediction = model_svm.predict(X_test)
accuracy = metrics.accuracy_score(y_test, prediction.round())

accuracy
