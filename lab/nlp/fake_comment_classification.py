# -*- coding: utf-8 -*-
"""虛假評論分類.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MT11orF0l5OWbkIFFiNb57JdvQdfKAkk
"""

from google.colab import files
import json
import re

import pandas as pd
import numpy as np
from pandas.core.frame import DataFrame
from sklearn.utils import shuffle

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

!pip install transformers 
from transformers import BertTokenizer, BertModel,BertConfig

def upload_file_to_colab():
  uploaded = files.upload()
  for fileName in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes.'.format(name=fileName, length=len(uploaded[fileName])))

# 三星寫手門資料集
# upload train.json
print('Upload train.json please.\n')
upload_file_to_colab()

# upload test.json
print('Upload test.json please.\n')
upload_file_to_colab()

def sentence_regex(sentence):
  complie = re.compile(u"[\u4e00-\u9fa5]+")

  sentence = sentence.replace("\n", ' ')
  sentence = re.sub(u'[a-zA-Z0-9]', '', sentence)

  sentence = re.findall(complie, sentence) #一則評論只取中文字，分成多行
  return ''.join(sentence)

def remove_empty_row(df, col='text'):
  emptyTitleFIlter = (
        (df[col].isnull())|(df[col] == '')|(df[col] == '0')
        )
  df = df[~emptyTitleFIlter]
  return df

def label_to_spam(is_spam):
  if is_spam == True:
      return 1 
  return 0

def preprocessing_data(filePath):
    data = pd.read_json(filePath)

    contents = list()
    spamLabels = list()
    length = len(data)

    for idx in range(length):
      sentence = data['content'][idx]
      is_spam = data['is_spam'][idx]

      sentence = sentence_regex(sentence)
      contents.append(sentence)
      
      is_spam = label_to_spam(is_spam)
      spamLabels.append(is_spam)

    return contents, spamLabels

train_contents, train_spamLabels = preprocessing_data('train.json')
print('* Train File:\n\tFirst sentence:', train_contents[0], '\n\tFirst label:', train_spamLabels[0])

test_contents, test_spamLabels = preprocessing_data('test.json')
print('* Test File:\n\tFirst sentence:', test_contents[0], '\n\tFirst label:', test_spamLabels[0])

trainDict = {
    'text': train_contents,
    'label': train_spamLabels
}

trainDf = DataFrame(trainDict)
trainDf = remove_empty_row(trainDf, 'text')
trainDf.head()

testDict = {
    'text': test_contents,
    'label': test_spamLabels
}

testDf = DataFrame(testDict)
testDf = remove_empty_row(testDf, 'text')
testDf.head()

train_true_filter = trainDf['label'] == 0
train_fake_filter = trainDf['label'] == 1

test_true_filter = testDf['label'] == 0
test_fake_filter = testDf['label'] == 1


true_train_count = len(trainDf.loc[train_true_filter])
true_test_count = len(testDf.loc[test_true_filter])
print('True Comment Count:', true_train_count + true_test_count)

fake_train_count = len(trainDf.loc[train_fake_filter])
fake_test_count = len(testDf.loc[test_fake_filter])
print('Fake Comment Count:', fake_train_count + fake_test_count)

def concat_two_dataframe(df_1, df_2):
  concated = pd.concat([df_1, df_2], axis=0)
  concated = shuffle(concated)
  # reset index
  concated = concated.reset_index(drop=True)
  return concated

# concated train dataframe and test dataframe
concated_df = concat_two_dataframe(trainDf, testDf)

fake_label_filter = concated_df['label'] == 1
true_label_filter = concated_df['label'] == 0

fake_data = concated_df.loc[fake_label_filter]
true_data = concated_df.loc[true_label_filter]

print('True data total number:', len(true_data))
print('Fake data total number:', len(fake_data))

# bcz Fake's number less than true's.
# random sampling from True's data.
# set the ratio is 3:1
sampling_true_data = true_data.sample(
    n=len(fake_data) * 3,
    random_state=5487
  )

dataset = concat_two_dataframe(sampling_true_data, fake_data)
dataset.info()

texts = dataset['text'].values
labels= dataset['label'].values

# Token Embeddings
PRETRAINED_MODEL_NAME = "bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

def encode_sentence_by_bert(sentence):
  encoded_sentence = tokenizer.encode(
        sentence, # Sentence to encode.
        add_special_tokens = True, # add [CLS] and [SEP]
        truncation = True,
        max_length = 100 
    )
  return encoded_sentence

input_ids = list()

for sentence in texts:
  encoded_sentence = encode_sentence_by_bert(sentence)
  input_ids.append(encoded_sentence)

print('Number of sentence:', len(input_ids))
print('Length of encode sentence:', len(input_ids[0]))

from keras.preprocessing.sequence import pad_sequences
MAX_LEN = 100
input_ids = pad_sequences(
    input_ids, 
    maxlen= MAX_LEN, 
    dtype="long", 
    value=0, # padding by value 0
    truncating="post", # truncating from behind
    padding="post") # padding from behind

attention_masks = list()
for sentence in input_ids:
    att_mask = [int(token_id > 0) for token_id in sentence]
    attention_masks.append(att_mask)

inputs = torch.tensor(input_ids)
labels = torch.tensor(labels)
masks = torch.tensor(attention_masks)

batch_size = 8
# Create the DataLoader for our training set.
train_dataset = TensorDataset(inputs, masks, labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME, output_hidden_states=True)
model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, config=config)
model.cuda()

if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

train_label = list()
train_set = list()

count=0
for step, batch in enumerate(train_dataloader):
  batch_tensor = tuple(tensr.to(device) for tensr in batch)
  b_input_ids, b_input_mask, b_labels = batch_tensor

  outputs = model(b_input_ids, b_input_mask)

  last_layer = outputs[0] # last layer
  label_ids = b_labels.to('cpu').numpy() # real label
  train_label.append(np.array(label_ids))

  for state in last_layer:
    # state size: (100, 768)
    allWordVecs = list()
    for word_unit in state:
      # size (768)
      allWordVecs.append(word_unit.detach().cpu().numpy())
    train_set.append(allWordVecs)

labels = list()
for i in train_label:
    for j in i:
        labels.append(j)
labels = np.array(labels)
print('Length of Labels:', len(labels))

layer = np.array(train_set)
train_set = None
print(layer.shape)

# split dataset 
X_train, X_test, y_train, y_test = \
    train_test_split(
        layer, 
        labels,
        test_size=0.3,
        random_state=0,
        stratify=labels
    )

X_train = X_train[:, 0, :]
X_test = X_test[:, 0, :]
print(X_train.shape)

model_svm = SVC(kernel='rbf')
model_svm.fit(X_train, y_train)
prediction = model_svm.predict(X_test)
accuracy = metrics.accuracy_score(y_test, prediction.round())

accuracy